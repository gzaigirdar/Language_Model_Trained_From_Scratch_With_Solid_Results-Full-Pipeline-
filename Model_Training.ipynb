{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb39737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54a27d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gz/Documents/Final_version_SLM/.llm_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel class defined.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import torch \n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from Training_utils.buildModel import BuildModel\n",
    "from Training_utils.training_tools import Train_tools\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554311e0",
   "metadata": {},
   "source": [
    "Data and Tokenizer Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9203e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# loading tokenizer \n",
    "\n",
    "tokenizer_path = '../Full Pipeline(LLM)/Saved_tokenizer/t5_Tokinzer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ef1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproccesd data load\n",
    "path = '/home/gz/Documents/Full Pipeline(LLM)/Saved_Data/Combineed_Pairs_Dataset'\n",
    "dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddc4d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'text_sample'],\n",
       "    num_rows: 196774\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e94701",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa48b3c",
   "metadata": {},
   "source": [
    "helper Variables info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "622077eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "ignore_index = tokenizer.pad_token_type_id\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_len = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff029",
   "metadata": {},
   "source": [
    "DataLoader and Data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f568a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'text_sample'],\n",
       "    num_rows: 196774\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78466ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32101, 32103, 497, 3, 6, 3, 354, 603, 3, 6, 149, 81, 352, 21, 3, 9, 360, 36, 277, 227, 2634, 3, 58, 32100, 32104, 25, 214, 24, 19, 24873, 68, 19, 310, 59, 207, 21, 69, 4639, 3, 5, 32100]\n",
      "<start> <user> say , jim , how about going for a few beers after dinner ? <end> <bot> you know that is tempting but is really not good for our fitness . <end>\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['text_sample'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5e3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token=ignore_index, device=device, max_length=max_len, user_token=\"<user>\"):\n",
    "    inputs_list = []\n",
    "    target_list = []\n",
    "    truncated_seqs = []\n",
    "    \n",
    "    for item in batch:\n",
    "        inputIds = item['input_ids']\n",
    "        if max_length is not None and len(inputIds) > max_length:\n",
    "            inputIds = inputIds[:max_length-1] + [inputIds[-1]]\n",
    "        truncated_seqs.append(inputIds)\n",
    "\n",
    "    batch_max_len = max(len(seq) + 1 for seq in truncated_seqs)\n",
    "\n",
    "    for inputIds in truncated_seqs:\n",
    "        new_input = list(inputIds) + [pad_token]\n",
    "        padded = new_input + [pad_token] * (batch_max_len - len(new_input))\n",
    "\n",
    "        input_tensor = torch.tensor(padded[:-1], dtype=torch.long, device=device)\n",
    "        label_tensor = torch.tensor(padded[1:], dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "        inputs_list.append(input_tensor)\n",
    "        target_list.append(label_tensor)\n",
    "\n",
    "    input_ids = torch.stack(inputs_list)\n",
    "    labels = torch.stack(target_list)\n",
    "\n",
    "    return input_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ff8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "collate_fn_pre_loaded = partial(collate_fn,pad_token=ignore_index,device=device,max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing text column \n",
    "\n",
    "dataset = dataset.remove_columns(['text_sample'])\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d0c7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Portion: 192838\n",
      "val portion: 3740\n",
      "test portion: 196\n"
     ]
    }
   ],
   "source": [
    "train_percentage = int(len(dataset) * 0.98)\n",
    "test_percentage = int(len(dataset)* 0.001)\n",
    "val_percentage = len(dataset) - train_percentage - test_percentage\n",
    "\n",
    "print(f'train Portion: {train_percentage}')\n",
    "print(f'val portion: {val_percentage}')\n",
    "print(f'test portion: {test_percentage}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3351bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 192838\n",
      "Validation Samples: 3740\n",
      "Test Samples: 196\n"
     ]
    }
   ],
   "source": [
    "# datset split\n",
    "train_data = dataset.select(range(train_percentage))\n",
    "val_data = dataset.select(range(train_percentage, train_percentage+val_percentage))\n",
    "test_data = dataset.select(range(train_percentage+val_percentage,len(dataset)))\n",
    "print(f'Train Samples: {len(train_data)}')\n",
    "print(f'Validation Samples: {len(val_data)}')\n",
    "print(f'Test Samples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95129711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "Train_loader = DataLoader(train_data,batch_size=2,collate_fn=collate_fn_pre_loaded,shuffle=True,drop_last=False)\n",
    "Val_Loader = DataLoader(val_data,batch_size=2,collate_fn=collate_fn_pre_loaded,shuffle=False,drop_last=False)\n",
    "Test_loader = DataLoader(test_data,batch_size=4,shuffle=False,drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "338277cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96419\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(Train_loader)\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735921a",
   "metadata": {},
   "source": [
    "Model and Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d92cbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'D_Model': 420,\n",
    "    'Num_Heads': 6,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.001,    \n",
    "    'Vocab_size': vocab_size,\n",
    "    'FeedForward_size': 2000,\n",
    "    'Context_size':80\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "train_config = {\n",
    "        'learning_rate': 0.001, \n",
    "\n",
    "        'GradAccumulation_steps': 1,\n",
    "        'Weight_decay': 0.001,\n",
    "        'Epochs': 5,\n",
    "        'Label_smoothing': 0.001,\n",
    "        'warmupsteps_percentage':0.30,\n",
    "        'Num_batches': num_batches,\n",
    "        'ignore_index': ignore_index,\n",
    "    \n",
    "\n",
    "} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ef4857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating the model \n",
    "\n",
    "builder = BuildModel(model_type='GModel2')\n",
    "model = builder.createModel(config=model_config,Model_type='gmodel2')\n",
    "model.to(device)\n",
    "#model.load_state_dict(torch.load('/home/gz/Documents/Final_version_SLM/Saved_Models/Full_trained_Model/60t_41m_pairs.pth'))\n",
    "# Training tools\n",
    "loss_fn,optimzer,lr_scheduler = Train_tools(config=train_config).getTools(model.parameters())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aade9cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.343965\n"
     ]
    }
   ],
   "source": [
    "print(builder.get_total_params(in_millons=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b957289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler, loss_fn, Train_Loader, Val_Loader, Epochs, grad_step, pad_token, device):\n",
    "    #scaler = GradScaler()\n",
    "    for epoch in range(Epochs):\n",
    "        progress_bar = tqdm(Train_Loader, desc=f\"Epoch {epoch+1} training\", leave=False)\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for step, (input_ids, labels) in enumerate(progress_bar):\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "                logits = model(input_ids, pad_token)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            loss = loss / grad_step\n",
    "            loss.backward()  \n",
    "            if (step + 1) % grad_step == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                '''min_lr = 5e-5\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = max(g['lr'], min_lr)'''\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "        average_loss = total_loss / len(Train_Loader)\n",
    "        print(f'Average loss: epoch: {epoch+1}, average_training loss: {average_loss:.4f}')\n",
    "        ''' torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': average_loss,\n",
    "                \n",
    "            },\n",
    "            f\"{CheckPoint_Path}/checkpoint{epoch+1}.pth\"\n",
    "        )'''\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        eval_progress_bar = tqdm(Val_Loader, desc=f\"Epoch {epoch+1} Validation:\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for input_ids, labels in eval_progress_bar:\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):  # <-- change here\n",
    "                    logits = model(input_ids, pad_token)\n",
    "                    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                eval_progress_bar.set_postfix({'loss': val_loss / max(eval_progress_bar.n, 1)})\n",
    "        average_eval_loss = val_loss / len(Val_Loader)\n",
    "        perplexity = math.exp(average_eval_loss)\n",
    "        print(f'Average Evaluation Loss: {epoch+1} {average_eval_loss:.4f}')\n",
    "        print(f'Perplexity Score: {perplexity:.2f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current Learning Rate: {current_lr:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b08a1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 1, average_training loss: 3.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 1 2.6299\n",
      "Perplexity Score: 13.87\n",
      "Current Learning Rate: 0.00066667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 2, average_training loss: 2.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 2 2.3828\n",
      "Perplexity Score: 10.83\n",
      "Current Learning Rate: 0.00095048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 3, average_training loss: 2.2521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 3 2.1767\n",
      "Perplexity Score: 8.82\n",
      "Current Learning Rate: 0.00061126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 4, average_training loss: 1.9751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 4 1.9925\n",
      "Perplexity Score: 7.33\n",
      "Current Learning Rate: 0.00018825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 5, average_training loss: 1.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 5 1.9393\n",
      "Perplexity Score: 6.95\n",
      "Current Learning Rate: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train(model,optimzer,lr_scheduler,loss_fn,Epochs=train_config['Epochs'],Train_Loader=Train_loader,Val_Loader=Val_Loader,grad_step=train_config['GradAccumulation_steps'],pad_token=ignore_index,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1467802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been saved in folder /home/gz/Documents/Final_version_SLM/Saved_Models/Full_trained_Model\n",
      "Model weights saved at: /home/gz/Documents/Final_version_SLM/Saved_Models/Full_trained_Model/80t_41m_b2_wo_pre.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "save_folder = Path(f\"/home/gz/Documents/Final_version_SLM/Saved_Models/Full_trained_Model\")\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(),save_folder/f'80t_41m_b2_wo_pre.pth')\n",
    "\n",
    "print(f'model has been saved in folder {save_folder}')\n",
    "print(f'Model weights saved at: {save_folder}/80t_41m_b2_wo_pre.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "671bb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_path = \"/home/gz/Documents/Final_version_SLM/Saved_Models/Full_trained_Model/config_80T_41m_b2_wo_pre\"\n",
    "\n",
    "all_configs = {\n",
    "    \"model_config\": model_config,\n",
    "    \"train_config\": train_config\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(all_configs, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "636614ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=50, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Build the formatted prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} {tokenizer.eos_token} <bot>\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt',add_special_tokens=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 80:   # Your sequence limit\n",
    "            break\n",
    "\n",
    "        # Forward pass â€” only input_ids and pad_token needed\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        # Take last token's logits\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Greedy decode\n",
    "        next_id = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Stop on custom end token\n",
    "        if tokenizer.decode(next_id[0]) == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append new token\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    # Decode into text\n",
    "    text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    "    # Extract only bot response\n",
    "    print(text)\n",
    "    words = text.split()\n",
    "    if \"<bot>\" in words:\n",
    "        bot_idx = words.index(\"<bot>\")\n",
    "        words = words[bot_idx:]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e143ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=50, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    \n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} {tokenizer.eos_token} <bot> \"\n",
    "\n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    end_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 80:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        next_logits = logits[:, -1, :]\n",
    "        next_id = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if next_id.item() == end_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "    # return only bot response\n",
    "    if \"<bot>\" in text:\n",
    "        text = text.split(\"<bot>\", 1)[1]\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79bcf715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you're welcome.\n"
     ]
    }
   ],
   "source": [
    "text = \" i'm doing well . thanks for asking . \"  \n",
    "\n",
    "print(gen_text(model,tokenizer,prompt=text,max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bf10000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.findall(r\"\\w+(?:'\\w+)*|[^\\w\\s]\", text)\n",
    "    tokens = \" \".join(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07162539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: how are you ?\n",
      "output: i'm fine. how about you?\n",
      "______________\n",
      "input: what is your name ?\n",
      "output: my name is li lin.\n",
      "______________\n",
      "input: what time is it now ?\n",
      "output: it's 7 : 00 p. m.\n",
      "______________\n",
      "input: hey , what are you up to ?\n",
      "output: i'm just a little tired.\n",
      "______________\n",
      "input: hey , what's up ?\n",
      "output: nothing much.\n",
      "______________\n",
      "input: where do you work ?\n",
      "output: i work at a bank.\n",
      "______________\n",
      "input: do you like to watch sports ?\n",
      "output: i like watching football.\n",
      "______________\n",
      "input: where are you from ?\n",
      "output: i'm from taiwan.\n",
      "______________\n",
      "input: do you like chinese food ?\n",
      "output: yes, i do. i love it.\n",
      "______________\n",
      "input: are you a chatbot ?\n",
      "output: yes, i am.\n",
      "______________\n",
      "input: where do you live ?\n",
      "output: i live in san francisco.\n",
      "______________\n",
      "input: do you want to go get some food ?\n",
      "output: i'd like to, but i'm not sure if i'll get enough sleep.\n",
      "______________\n",
      "input: do you want to watch a movie ?\n",
      "output: i'd love to.\n",
      "______________\n",
      "input: tell me about yourself .\n",
      "output: i'm a teacher.\n",
      "______________\n",
      "input: i'm doing well . thanks for asking .\n",
      "output: you're welcome.\n",
      "______________\n"
     ]
    }
   ],
   "source": [
    "# List of example prompts\n",
    "prompts = [\n",
    "    \"how are you?\",\n",
    "    \"what is your name?\",\n",
    "    \"what time is it now?\",\n",
    "    'hey, what are you up to?',\n",
    "    \"hey, what's up? \",\n",
    "    \"where do you work?\",\n",
    "    \"do you like to watch sports?\",\n",
    "    \"where are you from ?\",\n",
    "    \"do you like chinese food?\",\n",
    "    \"are you a chatbot?\",\n",
    "    \"where do you live?\",\n",
    "     \"do you want to go get some food?\",\n",
    "     \"do you want to watch a movie ?\",\n",
    "    \"tell me about yourself.\",\n",
    "    \"I'm doing well. Thanks for asking.\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Array to store responses\n",
    "responses = []\n",
    "\n",
    "# Loop through prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    \n",
    "    prompt = clean_text(prompt)\n",
    "   \n",
    "    \n",
    "    output = gen_text(model, tokenizer, prompt)\n",
    "    responses.append(output)\n",
    "    print(f\"input: {prompt}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"______________\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccc4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
