{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bb39737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b54a27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch \n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from Training_utils.buildModel import BuildModel\n",
    "from Training_utils.training_tools import Train_tools\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554311e0",
   "metadata": {},
   "source": [
    "Data and Tokenizer Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9203e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenizer \n",
    "\n",
    "tokenizer_path = '../Full Pipeline(LLM)/Saved_tokenizer/t5_Tokinzer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16ef1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproccesd data load\n",
    "path = '/home/gz/Documents/Full Pipeline(LLM)/Saved_Data/processedDataset'\n",
    "dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78e94701",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa48b3c",
   "metadata": {},
   "source": [
    "helper Variables info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "622077eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "ignore_index = tokenizer.pad_token_type_id\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_len = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff029",
   "metadata": {},
   "source": [
    "DataLoader and Data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59f568a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'text_sample'],\n",
       "    num_rows: 104161\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78466ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32101, 32103, 497, 3, 6, 3, 354, 603, 3, 6, 149, 81, 352, 21, 3, 9, 360, 36, 277, 227, 2634, 3, 58, 32100, 32104, 25, 214, 24, 19, 24873, 68, 19, 310, 59, 207, 21, 69, 4639, 3, 5, 32100]\n",
      "<start> <user> say , jim , how about going for a few beers after dinner ? <end> <bot> you know that is tempting but is really not good for our fitness . <end>\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['text_sample'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b5e3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate fn \n",
    "\n",
    "def collate_fn(batch,pad_token=ignore_index,device=device,max_length=max_len):\n",
    "   \n",
    "    \n",
    "   \n",
    "\n",
    "    inputs_list = []\n",
    "    target_list = []\n",
    "    truncated_seqs = []\n",
    "    for item in batch:\n",
    "        inputIds = item['input_ids']\n",
    "        if max_length is not None and len(inputIds) > max_length:\n",
    "            inputIds = inputIds[:max_length-1] + [inputIds[-1]]\n",
    "        truncated_seqs.append(inputIds)\n",
    "\n",
    "    batch_max_len = max(len(seq) + 1 for seq in truncated_seqs)\n",
    "    \n",
    "    for inputIds in truncated_seqs:\n",
    "\n",
    "        new_input = list(inputIds) + [pad_token]\n",
    "       \n",
    "        padded = (\n",
    "            new_input + [pad_token] * (batch_max_len - len(new_input)))\n",
    "    \n",
    "        inputs_list.append(torch.tensor(padded[:-1],dtype=torch.long,device=device))\n",
    "        target_list.append(torch.tensor(padded[1:],dtype=torch.long,device=device))\n",
    "\n",
    "        \n",
    "    input_ids = torch.stack(inputs_list)\n",
    "    labels = torch.stack(target_list)\n",
    "\n",
    "\n",
    "    return (input_ids,\n",
    "            labels)\n",
    "   \n",
    "\n",
    "collate_fn_pre_loaded = partial(collate_fn,pad_token=ignore_index,device=device,max_length=285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4ff8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "collate_fn_pre_loaded = partial(collate_fn,pad_token=ignore_index,device=device,max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76c393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing text column \n",
    "\n",
    "dataset = dataset.remove_columns(['text_sample'])\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d0c7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Portion: 98952\n",
      "val portion: 5105\n",
      "test portion: 104\n"
     ]
    }
   ],
   "source": [
    "train_percentage = int(len(dataset) * 0.95)\n",
    "test_percentage = int(len(dataset)* 0.001)\n",
    "val_percentage = len(dataset) - train_percentage - test_percentage\n",
    "\n",
    "print(f'train Portion: {train_percentage}')\n",
    "print(f'val portion: {val_percentage}')\n",
    "print(f'test portion: {test_percentage}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3351bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 98952\n",
      "Validation Samples: 5105\n",
      "Test Samples: 104\n"
     ]
    }
   ],
   "source": [
    "# datset split\n",
    "train_data = dataset.select(range(train_percentage))\n",
    "val_data = dataset.select(range(train_percentage, train_percentage+val_percentage))\n",
    "test_data = dataset.select(range(train_percentage+val_percentage,len(dataset)))\n",
    "print(f'Train Samples: {len(train_data)}')\n",
    "print(f'Validation Samples: {len(val_data)}')\n",
    "print(f'Test Samples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95129711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "Train_loader = DataLoader(train_data,batch_size=4,collate_fn=collate_fn_pre_loaded,shuffle=True,drop_last=False)\n",
    "Val_Loader = DataLoader(val_data,batch_size=4,collate_fn=collate_fn_pre_loaded,shuffle=False,drop_last=False)\n",
    "Test_loader = DataLoader(test_data,batch_size=4,shuffle=False,drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "338277cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(Train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735921a",
   "metadata": {},
   "source": [
    "Model and Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d92cbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'D_Model': 420,\n",
    "    'Num_Heads': 6,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,    \n",
    "    'Vocab_size': vocab_size,\n",
    "    'FeedForward_size': 2000,\n",
    "    'Context_size':80\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "train_config = {\n",
    "        'learning_rate':0.0001, \n",
    "\n",
    "        'GradAccumulation_steps': 1,\n",
    "        'Weight_decay': 0.01,\n",
    "        'Epochs': 2,\n",
    "        'Label_smoothing': 0.00,\n",
    "        'warmupsteps_percentage':0.30,\n",
    "        'Num_batches': num_batches,\n",
    "        'ignore_index': ignore_index,\n",
    "    \n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ef4857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7277/3188294650.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "builder = BuildModel(model_type='GModel2')\n",
    "model = builder.createModel(config=model_config, Model_type='gmodel2')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "state = torch.load(\n",
    "    '/home/gz/Documents/Full Pipeline(LLM)/Saved_Models/45m_285T_Pretrained/45m285T_pretrained.pth',\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# change pe for finetuning \n",
    "max_len = model_config[\"Context_size\"]  \n",
    "state[\"positional_encoding.pe\"] = state[\"positional_encoding.pe\"][:, :max_len, :]\n",
    "\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aade9cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.343965\n"
     ]
    }
   ],
   "source": [
    "print(builder.get_total_params(in_millons=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e8b580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding.weight\n",
      "transformer_blocks.0.Attention.WQ.weight\n",
      "transformer_blocks.0.Attention.WQ.bias\n",
      "transformer_blocks.0.Attention.WK.weight\n",
      "transformer_blocks.0.Attention.WK.bias\n",
      "transformer_blocks.0.Attention.WV.weight\n",
      "transformer_blocks.0.Attention.WV.bias\n",
      "transformer_blocks.0.Attention.Final_layer.weight\n",
      "transformer_blocks.0.Attention.Final_layer.bias\n",
      "transformer_blocks.0.FeedForward.fc1.weight\n",
      "transformer_blocks.0.FeedForward.fc1.bias\n",
      "transformer_blocks.0.FeedForward.fc2.weight\n",
      "transformer_blocks.0.FeedForward.fc2.bias\n",
      "transformer_blocks.0.RMSNorm1.scale\n",
      "transformer_blocks.0.RMSNorm2.scale\n",
      "transformer_blocks.1.Attention.WQ.weight\n",
      "transformer_blocks.1.Attention.WQ.bias\n",
      "transformer_blocks.1.Attention.WK.weight\n",
      "transformer_blocks.1.Attention.WK.bias\n",
      "transformer_blocks.1.Attention.WV.weight\n",
      "transformer_blocks.1.Attention.WV.bias\n",
      "transformer_blocks.1.Attention.Final_layer.weight\n",
      "transformer_blocks.1.Attention.Final_layer.bias\n",
      "transformer_blocks.1.FeedForward.fc1.weight\n",
      "transformer_blocks.1.FeedForward.fc1.bias\n",
      "transformer_blocks.1.FeedForward.fc2.weight\n",
      "transformer_blocks.1.FeedForward.fc2.bias\n",
      "transformer_blocks.1.RMSNorm1.scale\n",
      "transformer_blocks.1.RMSNorm2.scale\n",
      "transformer_blocks.2.Attention.WQ.weight\n",
      "transformer_blocks.2.Attention.WQ.bias\n",
      "transformer_blocks.2.Attention.WK.weight\n",
      "transformer_blocks.2.Attention.WK.bias\n",
      "transformer_blocks.2.Attention.WV.weight\n",
      "transformer_blocks.2.Attention.WV.bias\n",
      "transformer_blocks.2.Attention.Final_layer.weight\n",
      "transformer_blocks.2.Attention.Final_layer.bias\n",
      "transformer_blocks.2.FeedForward.fc1.weight\n",
      "transformer_blocks.2.FeedForward.fc1.bias\n",
      "transformer_blocks.2.FeedForward.fc2.weight\n",
      "transformer_blocks.2.FeedForward.fc2.bias\n",
      "transformer_blocks.2.RMSNorm1.scale\n",
      "transformer_blocks.2.RMSNorm2.scale\n",
      "transformer_blocks.3.Attention.WQ.weight\n",
      "transformer_blocks.3.Attention.WQ.bias\n",
      "transformer_blocks.3.Attention.WK.weight\n",
      "transformer_blocks.3.Attention.WK.bias\n",
      "transformer_blocks.3.Attention.WV.weight\n",
      "transformer_blocks.3.Attention.WV.bias\n",
      "transformer_blocks.3.Attention.Final_layer.weight\n",
      "transformer_blocks.3.Attention.Final_layer.bias\n",
      "transformer_blocks.3.FeedForward.fc1.weight\n",
      "transformer_blocks.3.FeedForward.fc1.bias\n",
      "transformer_blocks.3.FeedForward.fc2.weight\n",
      "transformer_blocks.3.FeedForward.fc2.bias\n",
      "transformer_blocks.3.RMSNorm1.scale\n",
      "transformer_blocks.3.RMSNorm2.scale\n",
      "transformer_blocks.4.Attention.WQ.weight\n",
      "transformer_blocks.4.Attention.WQ.bias\n",
      "transformer_blocks.4.Attention.WK.weight\n",
      "transformer_blocks.4.Attention.WK.bias\n",
      "transformer_blocks.4.Attention.WV.weight\n",
      "transformer_blocks.4.Attention.WV.bias\n",
      "transformer_blocks.4.Attention.Final_layer.weight\n",
      "transformer_blocks.4.Attention.Final_layer.bias\n",
      "transformer_blocks.4.FeedForward.fc1.weight\n",
      "transformer_blocks.4.FeedForward.fc1.bias\n",
      "transformer_blocks.4.FeedForward.fc2.weight\n",
      "transformer_blocks.4.FeedForward.fc2.bias\n",
      "transformer_blocks.4.RMSNorm1.scale\n",
      "transformer_blocks.4.RMSNorm2.scale\n",
      "transformer_blocks.5.Attention.WQ.weight\n",
      "transformer_blocks.5.Attention.WQ.bias\n",
      "transformer_blocks.5.Attention.WK.weight\n",
      "transformer_blocks.5.Attention.WK.bias\n",
      "transformer_blocks.5.Attention.WV.weight\n",
      "transformer_blocks.5.Attention.WV.bias\n",
      "transformer_blocks.5.Attention.Final_layer.weight\n",
      "transformer_blocks.5.Attention.Final_layer.bias\n",
      "transformer_blocks.5.FeedForward.fc1.weight\n",
      "transformer_blocks.5.FeedForward.fc1.bias\n",
      "transformer_blocks.5.FeedForward.fc2.weight\n",
      "transformer_blocks.5.FeedForward.fc2.bias\n",
      "transformer_blocks.5.RMSNorm1.scale\n",
      "transformer_blocks.5.RMSNorm2.scale\n",
      "final_norm.scale\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f959e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"token_embedding\"):\n",
    "        param.requires_grad = False\n",
    "    elif any(name.startswith(f\"transformer_blocks.{i}.\") for i in range(3)):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9a9aedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  \n",
    "    if any(name.startswith(f\"transformer_blocks.{i}.\") for i in range(3)):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01340565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training tools\n",
    "\n",
    "loss_fn,optimzer,lr_scheduler = Train_tools(config=train_config).getTools(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b957289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler, loss_fn, Train_Loader, Val_Loader, Epochs, grad_step, pad_token, device):\n",
    "    #scaler = GradScaler()\n",
    "    for epoch in range(Epochs):\n",
    "        progress_bar = tqdm(Train_Loader, desc=f\"Epoch {epoch+1} training\", leave=False)\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for step, (input_ids, labels) in enumerate(progress_bar):\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "                logits = model(input_ids, pad_token)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            loss = loss / grad_step\n",
    "            loss.backward()  \n",
    "            if (step + 1) % grad_step == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                '''min_lr = 5e-5\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = max(g['lr'], min_lr)'''\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "        average_loss = total_loss / len(Train_Loader)\n",
    "        print(f'Average loss: epoch: {epoch+1}, average_training loss: {average_loss:.4f}')\n",
    "        ''' torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': average_loss,\n",
    "                \n",
    "            },\n",
    "            f\"{CheckPoint_Path}/checkpoint{epoch+1}.pth\"\n",
    "        )'''\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        eval_progress_bar = tqdm(Val_Loader, desc=f\"Epoch {epoch+1} Validation:\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for input_ids, labels in eval_progress_bar:\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):  # <-- change here\n",
    "                    logits = model(input_ids, pad_token)\n",
    "                    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                eval_progress_bar.set_postfix({'loss': val_loss / max(eval_progress_bar.n, 1)})\n",
    "        average_eval_loss = val_loss / len(Val_Loader)\n",
    "        perplexity = math.exp(average_eval_loss)\n",
    "        print(f'Average Evaluation Loss: {epoch+1} {average_eval_loss:.4f}')\n",
    "        print(f'Perplexity Score: {perplexity:.2f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current Learning Rate: {current_lr:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b08a1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 1, average_training loss: 2.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 1 2.4672\n",
      "Perplexity Score: 11.79\n",
      "Current Learning Rate: 0.00008117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 2, average_training loss: 2.4330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 2 2.3672\n",
      "Perplexity Score: 10.67\n",
      "Current Learning Rate: 0.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train(model,optimzer,lr_scheduler,loss_fn,Epochs=train_config['Epochs'],Train_Loader=Train_loader,Val_Loader=Val_Loader,grad_step=train_config['GradAccumulation_steps'],pad_token=ignore_index,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1467802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been saved in folder ../Full Pipeline(LLM)/Saved_Models/Model_2\n",
      "Model weights saved at: ../Full Pipeline(LLM)/Saved_Models/Model_2/Ghat__60m_v2.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "save_folder = Path(f\"../Full Pipeline(LLM)/Saved_Models/Model_2\")\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(),save_folder/f'Gchat_v2_60m.pth')\n",
    "\n",
    "print(f'model has been saved in folder {save_folder}')\n",
    "print(f'Model weights saved at: {save_folder}/Ghat__60m_v2.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "671bb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_path = \"../Full Pipeline(LLM)/Saved_Models/Model_2/Gchat_v2_60m_info.json\"\n",
    "\n",
    "all_configs = {\n",
    "    \"model_config\": model_config,\n",
    "    \"train_config\": train_config\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(all_configs, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f3d38aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=285, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    prompt = f\"{tokenizer.bos_token} {prompt} \"\n",
    "    \n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt',add_special_tokens=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 285:   # Your sequence limit\n",
    "            break\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "       \n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "\n",
    "        next_id = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "        if tokenizer.decode(next_id[0]) == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append new token\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    # Decode into text\n",
    "    text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef79d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=50, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    \n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} {tokenizer.eos_token} <bot> \"\n",
    "\n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    end_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 80:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        next_logits = logits[:, -1, :]\n",
    "        next_id = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if next_id.item() == end_id:\n",
    "            break\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "\n",
    "    # return only bot response\n",
    "    if \"<bot>\" in text:\n",
    "        text = text.split(\"<bot>\", 1)[1]\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89e5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bf10000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.findall(r\"\\w+(?:'\\w+)*|[^\\w\\s]\", text)\n",
    "    \n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07162539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: how are you ?\n",
      "output: i'm fine.\n",
      "______________\n",
      "input: what is your name ?\n",
      "output: my name is liu.\n",
      "______________\n",
      "input: what time is it now ?\n",
      "output: it's a ten - minute.\n",
      "______________\n",
      "input: hey , what are you up to ?\n",
      "output: i'm just looking for a new hobby.\n",
      "______________\n",
      "input: hi , what's up ?\n",
      "output: i'm just looking for a new job.\n",
      "______________\n",
      "input: where do you work ?\n",
      "output: i work in a company.\n",
      "______________\n",
      "input: do you like to watch sports ?\n",
      "output: i like to watch sports.\n",
      "______________\n",
      "input: where are you from ?\n",
      "output: i'm from london.\n",
      "______________\n",
      "input: do you like chinese food ?\n",
      "output: yes, i do.\n",
      "______________\n",
      "input: are you a chatbot ?\n",
      "output: yes, i am.\n",
      "______________\n",
      "input: where do you live ?\n",
      "output: i live in a small town.\n",
      "______________\n",
      "input: do you want to go get some food ?\n",
      "output: i'd like to, but i'm not sure if i can.\n",
      "______________\n",
      "input: do you want to watch a movie ?\n",
      "output: i'm not sure.\n",
      "______________\n",
      "input: tell me about yourself .\n",
      "output: i'm glad you're welcome.\n",
      "______________\n"
     ]
    }
   ],
   "source": [
    "# List of example prompts\n",
    "prompts = [\n",
    "    \"how are you?\",\n",
    "    \"what is your name?\",\n",
    "    \"what time is it now?\",\n",
    "    'hey, what are you up to?',\n",
    "    \"hi, what's up? \",\n",
    "    \"where do you work?\",\n",
    "    \"do you like to watch sports?\",\n",
    "    \"where are you from ?\",\n",
    "    \"do you like chinese food?\",\n",
    "    \"are you a chatbot?\",\n",
    "    \"where do you live?\",\n",
    "     \"do you want to go get some food?\",\n",
    "     \"do you want to watch a movie ?\",\n",
    "    \"tell me about yourself.\",\n",
    "]\n",
    "\n",
    "# Array to store responses\n",
    "responses = []\n",
    "\n",
    "# Loop through prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    \n",
    "    prompt = clean_text(prompt)\n",
    "   \n",
    "    \n",
    "    output = gen_text(model, tokenizer, prompt)\n",
    "    responses.append(output)\n",
    "    print(f\"input: {prompt}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"______________\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: 6 layers, 6 heads, d_model=300, d_ff=800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10229/4106139349.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(f\"/home/gz/Documents/LLm_model/latest_with_new_tokenizer/model_24.3m.bin\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "model2_config = {\n",
    "    'D_Model': 300,\n",
    "    'Num_Heads': 6,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,\n",
    "    'Vocab_size':32105,\n",
    "    'FeedForward_size': 800,\n",
    "    'Context_size':100\n",
    "\n",
    "    \n",
    "}\n",
    "model2_config = {\n",
    "    'D_Model': 300,\n",
    "    'Num_Heads': 6,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,\n",
    "    'Vocab_size':32105,\n",
    "    'FeedForward_size': 800,\n",
    "    'Context_size':100\n",
    "\n",
    "    \n",
    "}\n",
    "model2 = BuildModel().createModel(model2_config)\n",
    "save_folder = \"/home/gz/Documents/LM_CONV_CHATBOT/latest_with_new_tokenizer\"\n",
    "\n",
    "model2.load_state_dict(torch.load(f\"/home/gz/Documents/LLm_model/latest_with_new_tokenizer/model_24.3m.bin\"))\n",
    "model2.eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bba393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe6ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_tokens_from_context(text, model, tokenizer, device='cuda', top_k=5):\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    #model.eval()\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {text} <sep> <bot> \"\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, pad_token=0)\n",
    "    \n",
    "    # Take logits for the last token\n",
    "    last_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "    probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "    # Get top K tokens\n",
    "    top_probs, top_ids = torch.topk(probs, k=top_k, dim=-1)\n",
    "    \n",
    "    # Map token IDs to readable text\n",
    "    top_tokens = [tokenizer.decode([tid.item()]) for tid in top_ids[0]]\n",
    "    \n",
    "    for i, (token, prob) in enumerate(zip(top_tokens, top_probs[0].tolist())):\n",
    "        print(f\"{i+1}: Token='{token}', Probability={prob:.4f}\")\n",
    "    \n",
    "    # Return the most probable token\n",
    "    return top_tokens[0], top_ids[0, 0].item(), top_probs[0, 0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e870b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Token='<user>', Probability=0.9512\n",
      "2: Token='<bot>', Probability=0.0002\n",
      "3: Token='hir', Probability=0.0000\n",
      "4: Token='men', Probability=0.0000\n",
      "5: Token='nes', Probability=0.0000\n",
      "Predicted token: <user>, ID: 32103, Prob: 0.9512\n"
     ]
    }
   ],
   "source": [
    "top_token, top_id, top_prob = top_tokens_from_context(\"how are you?\", model2, tokenizer)\n",
    "print(f\"Predicted token: {top_token}, ID: {top_id}, Prob: {top_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a554ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9212a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8830, 0.5982, 0.5724, 0.1700, 0.8080, 0.3550, 0.0067, 0.4760, 0.2647,\n",
       "         0.9305]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = x.unsqueeze(0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35da12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(4).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87853d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1402, 0.3122, 0.3962, 0.3240]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc3b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.cat([x,z],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567dce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8830, 0.5982, 0.5724, 0.1700, 0.8080, 0.3550, 0.0067, 0.4760, 0.2647,\n",
       "         0.9305, 0.1402, 0.3122, 0.3962, 0.3240]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccc4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
