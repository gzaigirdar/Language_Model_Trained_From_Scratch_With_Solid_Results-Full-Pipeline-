{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f705b8b7",
   "metadata": {},
   "source": [
    "importing all the statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1964e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea6615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gz/Documents/Full Pipeline(LLM)/.llm_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel class defined.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import torch \n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from Training_utils.buildModel import BuildModel\n",
    "from Training_utils.training_tools import Train_tools\n",
    "from functools import partial\n",
    "from torch.amp import GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b102f4",
   "metadata": {},
   "source": [
    "Tokenizer and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f63c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# loading tokenizer \n",
    "\n",
    "tokenizer_path = '../Saved_tokenizer/t5_Tokinzer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677241e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproccesd data load\n",
    "path = \"../Saved_Data/Tiny_chat_pretrained_dataset\"\n",
    "dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f243420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f095848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c196b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(row['input_ids']) for row in dataset]\n",
    "\n",
    "# Compute stats\n",
    "avg_len = sum(lengths) / len(lengths)\n",
    "min_len = min(lengths)\n",
    "max_len = max(lengths)\n",
    "\n",
    "print(f\"Number of sequences: {len(dataset)}\")\n",
    "print(f\"Average tokens per sequence: {avg_len:.1f}\")\n",
    "print(f\"Minimum tokens: {min_len}\")\n",
    "print(f\"Maximum tokens: {max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197d5a8",
   "metadata": {},
   "source": [
    " helper variables def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3069e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "ignore_index = tokenizer.pad_token_type_id\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134b03",
   "metadata": {},
   "source": [
    "Dataloader and data spliting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c08bc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate fn \n",
    "\n",
    "def collate_fn(batch,pad_token=ignore_index,device=device,max_length=max_len):\n",
    "   \n",
    "    \n",
    "   \n",
    "\n",
    "    inputs_list = []\n",
    "    target_list = []\n",
    "    truncated_seqs = []\n",
    "    for item in batch:\n",
    "        inputIds = item['input_ids']\n",
    "        if max_length is not None and len(inputIds) > max_length:\n",
    "            inputIds = inputIds[:max_length-1] + [inputIds[-1]]\n",
    "        truncated_seqs.append(inputIds)\n",
    "\n",
    "    batch_max_len = max(len(seq) + 1 for seq in truncated_seqs)\n",
    "    \n",
    "    for inputIds in truncated_seqs:\n",
    "\n",
    "        new_input = list(inputIds) + [pad_token]\n",
    "       \n",
    "        padded = (\n",
    "            new_input + [pad_token] * (batch_max_len - len(new_input)))\n",
    "    \n",
    "        inputs_list.append(torch.tensor(padded[:-1],dtype=torch.long,device=device))\n",
    "        target_list.append(torch.tensor(padded[1:],dtype=torch.long,device=device))\n",
    "\n",
    "        \n",
    "    input_ids = torch.stack(inputs_list)\n",
    "    labels = torch.stack(target_list)\n",
    "\n",
    "\n",
    "    return (input_ids,\n",
    "            labels)\n",
    "   \n",
    "\n",
    "collate_fn_pre_loaded = partial(collate_fn,pad_token=ignore_index,device=device,max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fbcc954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples: 980000\n",
      "validation samples: 20000\n"
     ]
    }
   ],
   "source": [
    "train_percentage = int(len(dataset) * 0.98)\n",
    "\n",
    "train_data = dataset.select(range(train_percentage))\n",
    "val_data = dataset.select(range(len(train_data),len(dataset)))\n",
    "print(f'training samples: {len(train_data)}')\n",
    "print(f'validation samples: {len(val_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fae1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "Train_loader = DataLoader(train_data,batch_size=16,collate_fn=collate_fn_pre_loaded,shuffle=True,drop_last=False)\n",
    "Val_Loader = DataLoader(val_data,batch_size=16,collate_fn=collate_fn_pre_loaded,shuffle=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e7a8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61250\n"
     ]
    }
   ],
   "source": [
    "# num of batches\n",
    "num_batches = len(Train_loader)\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a50871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_config = {\\n        'learning_rate': 0.001, \\n\\n        'GradAccumulation_steps': 1,\\n        'Weight_decay': 0.05,\\n        'Epochs': 1,\\n        'Label_smoothing': 0.05,\\n        'warmupsteps_percentage':0.30,\\n        'Num_batches': num_batches,\\n        'ignore_index': ignore_index,\\n\\n\\n} \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = {\n",
    "    'D_Model': 420,\n",
    "    'Num_Heads': 6,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,    \n",
    "    'Vocab_size': vocab_size,\n",
    "    'FeedForward_size': 2000,\n",
    "    'Context_size':200\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "'''train_config = {\n",
    "        'learning_rate': 0.001, \n",
    "\n",
    "        'GradAccumulation_steps': 1,\n",
    "        'Weight_decay': 0.05,\n",
    "        'Epochs': 1,\n",
    "        'Label_smoothing': 0.05,\n",
    "        'warmupsteps_percentage':0.30,\n",
    "        'Num_batches': num_batches,\n",
    "        'ignore_index': ignore_index,\n",
    "    \n",
    "\n",
    "} '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84425d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10272/1605091033.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/gz/Documents/Full Pipeline(LLM)/Saved_Models/Tiny_Chat_41m_Pretrained/41m200T_pretrained_Tchat.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crating the model \n",
    "\n",
    "builder = BuildModel(model_type='GModel2')\n",
    "model = builder.createModel(config=model_config,Model_type='gmodel2')\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/home/gz/Documents/Full Pipeline(LLM)/Saved_Models/Tiny_Chat_41m_Pretrained/41m200T_pretrained_Tchat.pth'))\n",
    "# Training tools\n",
    "#loss_fn,optimzer,lr_scheduler = Train_tools(config=train_config).getTools(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6eeeb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.343965\n"
     ]
    }
   ],
   "source": [
    "print(builder.get_total_params(in_millons=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e2ab0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "CheckPoint_Path = '../Full Pipeline(LLM)/Saved_Models/Tiny_Chat_PreTrained/CheckPoints'\n",
    "os.path.exists(CheckPoint_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d69ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler, loss_fn, Train_Loader, Val_Loader, Epochs, grad_step, pad_token, device, CheckPoint_Path=CheckPoint_Path):\n",
    "    #scaler = GradScaler()\n",
    "    for epoch in range(Epochs):\n",
    "        progress_bar = tqdm(Train_Loader, desc=f\"Epoch {epoch+1} training\", leave=False)\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for step, (input_ids, labels) in enumerate(progress_bar):\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "                logits = model(input_ids, pad_token)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            loss = loss / grad_step\n",
    "            loss.backward()  \n",
    "            if (step + 1) % grad_step == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                min_lr = 5e-5\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = max(g['lr'], min_lr)\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "        average_loss = total_loss / len(Train_Loader)\n",
    "        print(f'Average loss: epoch: {epoch+1}, average_training loss: {average_loss:.4f}')\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': average_loss,\n",
    "                \n",
    "            },\n",
    "            f\"{CheckPoint_Path}/checkpoint{epoch+1}.pth\"\n",
    "        )\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        eval_progress_bar = tqdm(Val_Loader, desc=f\"Epoch {epoch+1} Validation:\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for input_ids, labels in eval_progress_bar:\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):  # <-- change here\n",
    "                    logits = model(input_ids, pad_token)\n",
    "                    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                eval_progress_bar.set_postfix({'loss': val_loss / max(eval_progress_bar.n, 1)})\n",
    "        average_eval_loss = val_loss / len(Val_Loader)\n",
    "        perplexity = math.exp(average_eval_loss)\n",
    "        print(f'Average Evaluation Loss: {epoch+1} {average_eval_loss:.4f}')\n",
    "        print(f'Perplexity Score: {perplexity:.2f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current Learning Rate: {current_lr:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b87d6c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 1, average_training loss: 2.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 1 2.0417\n",
      "Perplexity Score: 7.70\n",
      "Current Learning Rate: 0.00005000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train(model,optimzer,lr_scheduler,loss_fn,Epochs=train_config['Epochs'],Train_Loader=Train_loader,Val_Loader=Val_Loader,grad_step=train_config['GradAccumulation_steps'],pad_token=ignore_index,device=device,CheckPoint_Path=CheckPoint_Path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47c985f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "def Save_model(model_name,folder_name,model):\n",
    "        \n",
    "        save_folder = Path(f\"../Full Pipeline(LLM)/Saved_Models/{folder_name}\")\n",
    "        save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(model.state_dict(),save_folder/f'{model_name}.pth')\n",
    "\n",
    "        print(f'model has been saved in folder {save_folder}')\n",
    "        print(f'Model weights saved at: {save_folder}/{model_name}.pth')\n",
    "def save_config(folder_name,file_name,model_config,train_config):\n",
    "\n",
    "\n",
    "        config_path = f\"../Full Pipeline(LLM)/Saved_Models/{folder_name}/{file_name}.json\"\n",
    "\n",
    "        all_configs = {\n",
    "                \"model_config\": model_config,\n",
    "                \"train_config\": train_config\n",
    "        }\n",
    "\n",
    "        with open(config_path, 'w') as f:\n",
    "                json.dump(all_configs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73dbd064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been saved in folder ../Full Pipeline(LLM)/Saved_Models/Tiny_Chat_41m_Pretrained\n",
      "Model weights saved at: ../Full Pipeline(LLM)/Saved_Models/Tiny_Chat_41m_Pretrained/41m200T_pretrained_Tchat.pth\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "Save_model(folder_name='Tiny_Chat_41m_Pretrained',model_name='41m200T_pretrained_Tchat',model=model)\n",
    "#builder.save_config(folder_name='PreTrain_1',model_config=model_config,train_config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e13be195",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_config(folder_name='Tiny_Chat_41m_Pretrained',file_name='Pretrained_Tchat_41m_config',model_config=model_config,train_config=train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed796d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=100, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Build the formatted prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} <bot> \"\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt',add_special_tokens=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 285:   # Your sequence limit\n",
    "            break\n",
    "\n",
    "        # Forward pass â€” only input_ids and pad_token needed\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        # Take last token's logits\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Greedy decode\n",
    "        next_id = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Stop on custom end token\n",
    "        if tokenizer.decode(next_id[0]) == \"<end>\" or tokenizer.decode(next_id[0]) == \"<user>\" :\n",
    "            break\n",
    "\n",
    "        # Append new token\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    # Decode into text\n",
    "    text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    "    # Extract only bot response\n",
    "    words = text.split()\n",
    "    if \"<bot>\" in words:\n",
    "        bot_idx = words.index(\"<bot>\")\n",
    "        words = words[bot_idx:]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7887109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start><user> why do people fall in love so fast?<bot> it can feel very unfair when others do not show respect for others.\n"
     ]
    }
   ],
   "source": [
    "text = \" why do people fall in love so fast ? \" \n",
    "\n",
    "print(gen_text(model,tokenizer,prompt=text,max_tokens=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "026ed0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: hey how are you ?\n",
      "output: <start><user> hey how are you?<bot> i am feeling very happy today, thank you for asking.\n",
      "______________\n",
      "input: what is the meaning of love ?\n",
      "output: <start><user> what is the meaning of love?<bot> love is a strong feeling that brings people together in many ways.\n",
      "______________\n",
      "input: do you like watching movies ?\n",
      "output: <start><user> do you like watching movies?<bot> i enjoy watching movies that have a wide range of interesting characters and plots.\n",
      "______________\n",
      "input: do you like reading books or watching movies better ?\n",
      "output: <start><user> do you like reading books or watching movies better?<bot> i enjoy reading books because they take me to different worlds.\n",
      "______________\n",
      "input: tell me the best way to learn to read .\n",
      "output: <start><user> tell me the best way to learn to read.<bot> that sounds interesting and surprising. what kind of book do you enjoy reading?\n",
      "______________\n",
      "input: i'm doing well , how about you ? .\n",
      "output: <start><user> i'm doing well, how about you?.<bot> i am feeling a bit bad today, thank you for asking.\n",
      "______________\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.findall(r\"\\w+(?:'\\w+)*|[^\\w\\s]\", text)\n",
    "    \n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "prompts = [\n",
    "    \"hey how are you?\",\n",
    "    \"what is the meaning of love ?\",\n",
    "    \"do you like watching movies ?\",\n",
    "    \"do you like reading books or watching movies better?\",\n",
    "    \" Tell me the best way to learn to read.\",\n",
    "    \"I'm doing well, how about you?.\"\n",
    "    \n",
    "]\n",
    "responses = [ ]\n",
    "for prompt in prompts:\n",
    "    \n",
    "    prompt = clean_text(prompt)\n",
    "   \n",
    "    \n",
    "    output = gen_text(model, tokenizer, prompt,max_tokens=200)\n",
    "    responses.append(output)\n",
    "    print(f\"input: {prompt}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"______________\")\n",
    " \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f9d180f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-5): 6 x TransformerBlock(\n",
       "    (Attention): MultiHeadAttention(\n",
       "      (WQ): Linear(in_features=420, out_features=420, bias=True)\n",
       "      (WK): Linear(in_features=420, out_features=420, bias=True)\n",
       "      (WV): Linear(in_features=420, out_features=420, bias=True)\n",
       "      (Final_layer): Linear(in_features=420, out_features=420, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (FeedForward): PointwiseFeedForward(\n",
       "      (fc1): Linear(in_features=420, out_features=2000, bias=True)\n",
       "      (fc2): Linear(in_features=2000, out_features=420, bias=True)\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (RMSNorm1): RMSNorm()\n",
       "    (RMSNorm2): RMSNorm()\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29114252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
