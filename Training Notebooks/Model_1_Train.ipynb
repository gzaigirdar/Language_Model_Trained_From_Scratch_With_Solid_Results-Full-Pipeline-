{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bb39737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54a27d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gz/Documents/Full Pipeline(LLM)/.llm_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel class defined.\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import torch \n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from Training_utils.buildModel import BuildModel\n",
    "from Training_utils.training_tools import Train_tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554311e0",
   "metadata": {},
   "source": [
    "Data and Tokenizer Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9203e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenizer \n",
    "\n",
    "tokenizer_path = '../Full Pipeline(LLM)/Saved_tokenizer/t5_Tokinzer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a52510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb2947f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32105"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16ef1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preproccesd data load\n",
    "path = '../Full Pipeline(LLM)/Saved_Data/processedDataset'\n",
    "dataset = load_from_disk(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26e24fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 44.29\n",
      "Max sequence length: 413\n",
      "Min sequence length: 11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: dataset is a list of dicts\n",
    "# dataset = [{'input_ids': [...]}, {'input_ids': [...]}, ...]\n",
    "\n",
    "lengths = [len(item['input_ids']) for item in dataset]\n",
    "avg_len = np.mean(lengths)\n",
    "max_len = np.max(lengths)\n",
    "min_len = np.min(lengths)\n",
    "\n",
    "print(f\"Average sequence length: {avg_len:.2f}\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Min sequence length: {min_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78e94701",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa48b3c",
   "metadata": {},
   "source": [
    "Tokens and vocab info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a358e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens info \n",
    "max_tokens = max(len(row['input_ids']) for row in dataset)\n",
    "min_tokens = min(len(row['input_ids']) for row in dataset)\n",
    "vocab_size = len(tokenizer)\n",
    "ignore_index = tokenizer.pad_token_type_id\n",
    "max_len = 100\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae5cf26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<end>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(32100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ae5c5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxTokens: 413\n",
      "min_Tokens: 11\n",
      "vocab_size: 32105\n"
     ]
    }
   ],
   "source": [
    "print(f'maxTokens: {max_tokens}')\n",
    "print(f'min_Tokens: {min_tokens}')\n",
    "print(f'vocab_size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbff029",
   "metadata": {},
   "source": [
    "DataLoader and Data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59f568a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'text_sample'],\n",
       "    num_rows: 95536\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78466ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32101, 32103, 497, 3, 6, 3, 354, 603, 3, 6, 149, 81, 352, 21, 3, 9, 360, 36, 277, 227, 2634, 3, 58, 32100, 32104, 25, 214, 24, 19, 24873, 68, 19, 310, 59, 207, 21, 69, 4639, 3, 5, 32100]\n",
      "<start> <user> say , jim , how about going for a few beers after dinner ? <end> <bot> you know that is tempting but is really not good for our fitness . \n"
     ]
    }
   ],
   "source": [
    "for sample in dataset:\n",
    "    print(sample['input_ids'])\n",
    "    print(sample['text_sample'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b5e3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch,pad_token=ignore_index,device=device,max_length=max_len):\n",
    "   \n",
    "    \n",
    "    # have to add extra token to input, due to shifting of the label to one to right.\n",
    "    batch_max_len = min(max(len(item['input_ids'])+1 for item in batch), max_length)\n",
    "\n",
    "    inputs_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "       \n",
    "        inputIds = item['input_ids']\n",
    "        new_input = list(inputIds) + [pad_token]\n",
    "       \n",
    "        padded = (\n",
    "            new_input + [pad_token] * (batch_max_len - len(new_input)))\n",
    "        if max_length is not None:\n",
    "            padded = padded[:max_length]\n",
    "        inputs_list.append(torch.tensor(padded[:-1],dtype=torch.long,device=device))\n",
    "        target_list.append(torch.tensor(padded[1:],dtype=torch.long,device=device))\n",
    "\n",
    "        \n",
    "    input_ids = torch.stack(inputs_list)\n",
    "    labels = torch.stack(target_list)\n",
    "\n",
    "\n",
    "    return (input_ids,\n",
    "            labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4ff8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "collate_fn_pre_loaded = partial(collate_fn,pad_token=ignore_index,device=device,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76c393bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing text column \n",
    "\n",
    "dataset = dataset.remove_columns(['text_sample'])\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Portion: 90759\n",
      "val portion: 3822\n",
      "test portion: 955\n"
     ]
    }
   ],
   "source": [
    "train_percentage = int(len(dataset) * 0.95)\n",
    "test_percentage = int(len(dataset)* 0.01)\n",
    "val_percentage = len(dataset) - train_percentage - test_percentage\n",
    "\n",
    "print(f'train Portion: {train_percentage}')\n",
    "print(f'val portion: {val_percentage}')\n",
    "print(f'test portion: {test_percentage}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3351bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 90759\n",
      "Validation Samples: 3822\n",
      "Test Samples: 955\n"
     ]
    }
   ],
   "source": [
    "# datset split\n",
    "train_data = dataset.select(range(train_percentage))\n",
    "val_data = dataset.select(range(train_percentage, train_percentage+val_percentage))\n",
    "test_data = dataset.select(range(train_percentage+val_percentage,len(dataset)))\n",
    "print(f'Train Samples: {len(train_data)}')\n",
    "print(f'Validation Samples: {len(val_data)}')\n",
    "print(f'Test Samples: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95129711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "Train_loader = DataLoader(train_data,batch_size=4,collate_fn=collate_fn_pre_loaded,shuffle=True,drop_last=False)\n",
    "Val_Loader = DataLoader(val_data,batch_size=4,collate_fn=collate_fn_pre_loaded,shuffle=False,drop_last=False)\n",
    "Test_loader = DataLoader(test_data,batch_size=4,shuffle=False,drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "338277cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(Train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735921a",
   "metadata": {},
   "source": [
    "Model and Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d92cbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'D_Model': 600,\n",
    "    'Num_Heads': 8,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,\n",
    "    'Vocab_size': vocab_size,\n",
    "    'FeedForward_size': 800,\n",
    "    'Context_size':100\n",
    "\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "        'learning_rate': 0.001, \n",
    "\n",
    "        'GradAccumulation_steps': 1,\n",
    "        'Weight_decay': 0.05,\n",
    "        'Epochs': 5,\n",
    "        'Label_smoothing': 0.05,\n",
    "        'warmupsteps_percentage':0.20,\n",
    "        'Num_batches': num_batches,\n",
    "        'ignore_index': ignore_index,\n",
    "    \n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: 6 layers, 8 heads, d_model=600, d_ff=800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GModel(\n",
       "  (token_embedding): Embedding(32105, 600)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (Attention): MultiHeadAttention(\n",
       "        (WQ): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (WK): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (WV): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (Final_layer): Linear(in_features=600, out_features=600, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (FeedForward): PointwiseFeedForward(\n",
       "        (fc1): Linear(in_features=600, out_features=800, bias=True)\n",
       "        (fc2): Linear(in_features=800, out_features=600, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (RMSNorm1): RMSNorm()\n",
       "      (RMSNorm2): RMSNorm()\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (fc): Linear(in_features=600, out_features=32105, bias=True)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crating the model \n",
    "\n",
    "builder = BuildModel()\n",
    "model = builder.createModel(config=model_config)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01340565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training tools\n",
    "\n",
    "loss_fn,optimzer,lr_scheduler = Train_tools(config=train_config).getTools(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b957289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def train(model, optimizer, lr_scheduler, loss_fn, Train_Loader, Val_Loader, Epochs, grad_step, pad_token, device):\n",
    "    for epoch in range(Epochs):\n",
    "        progress_bar = tqdm(Train_Loader, desc=f\"Epoch {epoch+1} training\", leave=False)\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for step, (input_ids, labels) in enumerate(progress_bar):\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(input_ids, pad_token)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loss = loss / grad_step\n",
    "            loss.backward()\n",
    "            if (step + 1) % grad_step == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "        average_loss = total_loss / len(Train_Loader)\n",
    "        print(f'Average loss: epoch: {epoch+1}, average_training loss: {average_loss:.4f}')\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        eval_progress_bar = tqdm(Val_Loader, desc=f\"Epoch {epoch+1} Validation:\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for input_ids, labels in eval_progress_bar:\n",
    "                input_ids = input_ids.to(device)\n",
    "                labels = labels.to(device)\n",
    "                logits = model(input_ids, pad_token)\n",
    "                logits = model(input_ids, pad_token)\n",
    "                loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "             \n",
    "                val_loss += loss.item()\n",
    "                eval_progress_bar.set_postfix({'loss': val_loss / max(eval_progress_bar.n, 1)})\n",
    "        average_eval_loss = val_loss / len(Val_Loader)\n",
    "        perplexity = math.exp(average_eval_loss)\n",
    "        print(f'average eva loss: {average_eval_loss:.4f}')\n",
    "        print(f'perplexity score: {perplexity:.2f}')\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'curent learning rate: {current_lr:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b08a1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 1, average_training loss: 3.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average eva loss: 3.2817\n",
      "perplexity score: 26.62\n",
      "curent learning rate: 0.00100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 2, average_training loss: 3.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average eva loss: 2.9834\n",
      "perplexity score: 19.76\n",
      "curent learning rate: 0.00075000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 3, average_training loss: 2.8835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average eva loss: 2.8129\n",
      "perplexity score: 16.66\n",
      "curent learning rate: 0.00050000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 4, average_training loss: 2.6477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average eva loss: 2.6096\n",
      "perplexity score: 13.59\n",
      "curent learning rate: 0.00025000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: epoch: 5, average_training loss: 2.3457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average eva loss: 2.4333\n",
      "perplexity score: 11.40\n",
      "curent learning rate: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "train(model,optimzer,lr_scheduler,loss_fn,Epochs=train_config['Epochs'],Train_Loader=Train_loader,Val_Loader=Val_Loader,grad_step=train_config['GradAccumulation_steps'],pad_token=ignore_index,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1467802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been saved in folder ../Full Pipeline(LLM)/Saved_Models/Model_1\n",
      "Model weights saved at: ../Full Pipeline(LLM)/Saved_Models/Model_1/Ghat_v1.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "save_folder = Path(f\"../Full Pipeline(LLM)/Saved_Models/Model_1\")\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(),save_folder/f'Gchat_v1.pth')\n",
    "\n",
    "print(f'model has been saved in folder {save_folder}')\n",
    "print(f'Model weights saved at: {save_folder}/Ghat_v1.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "671bb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# save config\n",
    "config_path = \"../Full Pipeline(LLM)/Saved_Models/Model_1/Gchat_v1_info.json\"\n",
    "configs_name = [model_config,train_config]\n",
    "with open(config_path,'w') as f:\n",
    "    for name in configs_name:\n",
    "        json.dump(name,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636614ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, prompt, max_tokens=25, pad_token=0, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Build the formatted prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} {tokenizer.eos_token} <bot>\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt',add_special_tokens=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 100:   # Your sequence limit\n",
    "            break\n",
    "\n",
    "        # Forward pass â€” only input_ids and pad_token needed\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        # Take last token's logits\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Greedy decode\n",
    "        next_id = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "        # Stop on custom end token\n",
    "        if tokenizer.decode(next_id[0]) == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append new token\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    # Decode into text\n",
    "    text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    "    # Extract only bot response\n",
    "    words = text.split()\n",
    "    if \"<bot>\" in words:\n",
    "        bot_idx = words.index(\"<bot>\")\n",
    "        words = words[bot_idx:]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "beca5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gen_text(model, tokenizer, prompt, max_tokens=25, pad_token=0, device=None,\n",
    "             temperature=None, top_k=None, top_p=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Build the formatted prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {prompt} {tokenizer.eos_token} <bot>\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        if generated_ids.size(1) >= 100:\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated_ids, pad_token=pad_token)\n",
    "\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature is not None:\n",
    "            next_logits = next_logits / temperature\n",
    "\n",
    "        # Top-k\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k_val = min(top_k, next_logits.size(-1))\n",
    "            values, _ = torch.topk(next_logits, top_k_val)\n",
    "            min_values = values[:, -1].unsqueeze(-1)\n",
    "            next_logits = torch.where(next_logits < min_values, torch.full_like(next_logits, -float('Inf')), next_logits)\n",
    "\n",
    "        # Top-p (nucleus)\n",
    "        if top_p is not None and 0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = 0\n",
    "            sorted_logits[sorted_indices_to_remove] = -float('Inf')\n",
    "            next_logits = torch.zeros_like(next_logits).scatter_(1, sorted_indices, sorted_logits)\n",
    "\n",
    "        # Sampling or greedy\n",
    "        if top_k is None and top_p is None and temperature is None:\n",
    "            next_id = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "        else:\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # shape [1]\n",
    "\n",
    "        # Stop on custom end token\n",
    "        if tokenizer.decode(next_id[0]) == \"<end>\":\n",
    "            break\n",
    "\n",
    "        # Append new token\n",
    "        generated_ids = torch.cat([generated_ids, next_id], dim=-1)\n",
    "\n",
    "    text = tokenizer.decode(generated_ids[0].tolist())\n",
    "\n",
    "    words = text.split()\n",
    "    if \"<bot>\" in words:\n",
    "        bot_idx = words.index(\"<bot>\")\n",
    "        words = words[bot_idx:]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4e15af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bot> i'm not sure.\n"
     ]
    }
   ],
   "source": [
    "text = \" what's up ? \" \n",
    "\n",
    "print(gen_text(model,tokenizer,prompt=text,max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "3bf10000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = re.findall(r\"\\w+(?:'\\w+)*|[^\\w\\s]\", text)\n",
    "    \n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "07162539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: how are you ?\n",
      "output: <bot> i'm fine. how about you?\n",
      "______________\n",
      "input: what is your name ?\n",
      "output: <bot> my name is john sandals.\n",
      "______________\n",
      "input: what time is it now ?\n",
      "output: <bot> it's ten o'clock.\n",
      "______________\n",
      "input: hey , what are you up to ?\n",
      "output: <bot> i'm just watching tv.\n",
      "______________\n",
      "input: hi , what's up ?\n",
      "output: <bot> i'm not feeling well. i'm just a little tired.\n",
      "______________\n",
      "input: where do you work ?\n",
      "output: <bot> i work at a polling place every year.\n",
      "______________\n",
      "input: do you like to watch sports ?\n",
      "output: <bot> i like football.\n",
      "______________\n",
      "input: where are you from ?\n",
      "output: <bot> i'm from guangzhou.\n",
      "______________\n",
      "input: do you like chinese food ?\n",
      "output: <bot> i like it very much.\n",
      "______________\n",
      "input: are you a chatbot ?\n",
      "output: <bot> yes, i am.\n",
      "______________\n",
      "input: where do you live ?\n",
      "output: <bot> i live in london.\n",
      "______________\n",
      "input: do you want to go get some food ?\n",
      "output: <bot> i'd like to, but i'm not sure if i could.\n",
      "______________\n",
      "input: do you want to watch a movie ?\n",
      "output: <bot> i don't know.\n",
      "______________\n",
      "input: tell me about yourself .\n",
      "output: <bot> i'm a little nervous.\n",
      "______________\n"
     ]
    }
   ],
   "source": [
    "# List of example prompts\n",
    "prompts = [\n",
    "    \"how are you?\",\n",
    "    \"what is your name?\",\n",
    "    \"what time is it now?\",\n",
    "    'hey, what are you up to ?',\n",
    "    \"hi, what's up? \",\n",
    "    \"where do you work?\",\n",
    "    \"do you like to watch sports ?\",\n",
    "    \"where are you from ?\",\n",
    "    \"do you like chinese food?\",\n",
    "    \"are you a chatbot?\",\n",
    "    \"where do you live?\",\n",
    "     \"do you want to go get some food?\",\n",
    "     \"do you want to watch a movie ?\",\n",
    "    \"tell me about yourself.\",\n",
    "]\n",
    "\n",
    "# Array to store responses\n",
    "responses = []\n",
    "\n",
    "# Loop through prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    \n",
    "    prompt = clean_text(prompt)\n",
    "   \n",
    "    \n",
    "    output = gen_text(model, tokenizer, prompt)\n",
    "    responses.append(output)\n",
    "    print(f\"input: {prompt}\")\n",
    "    print(f\"output: {output}\")\n",
    "    print(\"______________\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec881f",
   "metadata": {},
   "source": [
    "Model Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3e7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: 6 layers, 8 heads, d_model=600, d_ff=800\n",
      "52.988705\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8817/3182676116.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "vocab_size = len(tokenizer)\n",
    "model_config = {\n",
    "    'D_Model': 600,\n",
    "    'Num_Heads': 8,\n",
    "    'Num_Layers':6,\n",
    "    'Dropout': 0.05,\n",
    "    'Vocab_size': vocab_size,\n",
    "    'FeedForward_size': 800,\n",
    "    'Context_size':100\n",
    "\n",
    "}\n",
    "\n",
    "Device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "builder = BuildModel(model_type='GModel2')\n",
    "model = builder.createModel(model_config)\n",
    "path = \"../Full Pipeline(LLM)/Saved_Models/Model_1/Gchat_v1_53m.pth\"\n",
    "model.to(Device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "print(builder.get_total_params(in_millons=True))\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bba393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe6ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_tokens_from_context(text, model, tokenizer, device='cuda', top_k=5):\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    #model.eval()\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = f\"{tokenizer.bos_token} <user> {text} <sep> <bot> \"\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, pad_token=0)\n",
    "    \n",
    "    # Take logits for the last token\n",
    "    last_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "    probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "    # Get top K tokens\n",
    "    top_probs, top_ids = torch.topk(probs, k=top_k, dim=-1)\n",
    "    \n",
    "    # Map token IDs to readable text\n",
    "    top_tokens = [tokenizer.decode([tid.item()]) for tid in top_ids[0]]\n",
    "    \n",
    "    for i, (token, prob) in enumerate(zip(top_tokens, top_probs[0].tolist())):\n",
    "        print(f\"{i+1}: Token='{token}', Probability={prob:.4f}\")\n",
    "    \n",
    "    # Return the most probable token\n",
    "    return top_tokens[0], top_ids[0, 0].item(), top_probs[0, 0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e870b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Token='<user>', Probability=0.9512\n",
      "2: Token='<bot>', Probability=0.0002\n",
      "3: Token='hir', Probability=0.0000\n",
      "4: Token='men', Probability=0.0000\n",
      "5: Token='nes', Probability=0.0000\n",
      "Predicted token: <user>, ID: 32103, Prob: 0.9512\n"
     ]
    }
   ],
   "source": [
    "top_token, top_id, top_prob = top_tokens_from_context(\"how are you?\", model2, tokenizer)\n",
    "print(f\"Predicted token: {top_token}, ID: {top_id}, Prob: {top_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a554ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
