
Combined Dataset:
usage: The dataset was use to train the main model that acheive the best result in daily basic convo. 
Description:
Combined Dataset had 193k samples of single prompt and response pairs.
The samples were build from DailyDialog,General_Conversation_Mixed_Dataset,everyday-conversations-llama3.1-2k.chatbot Dataset(from kaggle),
and randmlogy picked 100k samples from 1 millon rows of tiny chat dataset.(see the credit section for information)


Story generator Dataset:
Description: Tiny stories dataset from hugging face(check credit section for more information)
usage: This dataset was used to train a model to generate short coherent stories(check Tiny_stories_41m_285_Pretrained.ipynb for training details).
All samples of tiny stories dataset were used to train a model to generate short stories.

Full Tiny Chat Dataset:
Description: Tiny chat dataset from hugging face(check credit section for more information)
usage: All one millon samples of  multiturn dialgues was used to train a basic conversational model(Check Tiny_chat_Data_Model.ipynb for training details).
conversation.

-------------------------------------------------------------------------------------------------------------------------------------------------
Dataset information and Credit Section:


1. DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset

usage: pairs of prompt and responses were extracted from the utterances in daily dialog dataset.

License
DailyDialog dataset is licensed under CC BY-NC-SA 4.0.
Link:
https://huggingface.co/datasets/roskoN/dailydialog

2. General_Conversation_Mixed_Dataset 
usage: pairs of prompt and responses were extracted from the dataset.
Link: https://huggingface.co/datasets/cosmosai471/General_Conversation_Mixed_Dataset

3. HuggingFaceTB/everyday-conversations-llama3.1-2k

usage: pairs of prompt and responses were extracted from the dataset.
Description: This dataset contains 2.2k multi-turn conversations generated by Llama-3.1-70B-Instruct. We ask the LLM to generate a simple multi-turn conversation, with 3-4 short exchanges, between a User and an AI Assistant about a certain topic.

credit: @misc{everydayconversations2024,
  author = {Hugging Face},
  title = {Everyday Conversations for LLMs},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k}}
}
link: https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k

____________________________________________
4. Tiny Chat from HuggingFace
Usage: single pairs of responses were extracted from the multi turn dialogues in the dataset.
Description: This dataset comprises 1,000,000 synthetically generated short chat conversations, created using a specialized version of GPT-4o (referred to as GPT-4o mini). The conversations are primarily constructed using BASIC (British Academic Scientific International Commercial) English words and grammar. However, to ensure the coherence and fluidity of the dialogues, some non-BASIC English words have been included selectivel
Link: https://huggingface.co/datasets/starhopp3r/TinyChat 

5. Tiny Stories from Huggingface
Usage: stories were used from dataset to create blocks of input data to train a story generator model. 
Description: Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.
Link: https://huggingface.co/datasets/roneneldan/TinyStories


7. Dataset for chatbot from kaggle 

usage: pairs of prompts and respnses were extracted from the dataset.
Link: https://www.kaggle.com/datasets/grafstor/simple-dialogs-for-chatbot

8. Custom samples
Description: harcoded prompt and respnses were created
samples are in in Saved_Data/harcoded_samples.py